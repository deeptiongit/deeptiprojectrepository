{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53dd9029-35a6-4817-950f-06f5fc4293a8",
   "metadata": {},
   "source": [
    "# MIAMI MEETING REPORT CHATBOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf2ca270-ab38-45ed-a147-bd9e9f8ee376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/kushawaha-deepti/.config/kdedefaults/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/kushawaha-deepti/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushawaha-deepti/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'role' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 95\u001b[0m\n\u001b[1;32m     87\u001b[0m git_config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers.git\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbranch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv4.37.0\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Creates Hugging Face estimator\u001b[39;00m\n\u001b[1;32m     90\u001b[0m huggingface_estimator \u001b[38;5;241m=\u001b[39m HuggingFace(\n\u001b[1;32m     91\u001b[0m     entry_point\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_qa.py\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     92\u001b[0m     source_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     93\u001b[0m     instance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml.p3.2xlarge\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     94\u001b[0m     instance_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m---> 95\u001b[0m     role\u001b[38;5;241m=\u001b[39mrole,\n\u001b[1;32m     96\u001b[0m     git_config\u001b[38;5;241m=\u001b[39mgit_config,\n\u001b[1;32m     97\u001b[0m     transformers_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.37.0\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     98\u001b[0m     pytorch_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2.1.0\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     99\u001b[0m     py_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpy310\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    100\u001b[0m     hyperparameters\u001b[38;5;241m=\u001b[39mhyperparameters\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Start the training job\u001b[39;00m\n\u001b[1;32m    104\u001b[0m huggingface_estimator\u001b[38;5;241m.\u001b[39mfit()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'role' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import atexit\n",
    "import shutil\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "from transformers import get_scheduler \n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Load JSON data from a local file\n",
    "with open('/home/kushawaha-deepti/Downloads/miami_data.json') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Flatten the JSON data into a DataFrame\n",
    "df = json_normalize(json_data)\n",
    "df = df.T\n",
    "df.columns = ['REPORTS']\n",
    "\n",
    "# Define lists to store parsed results\n",
    "dates = []\n",
    "meeting_titles = []\n",
    "times = []\n",
    "present_members_list = []\n",
    "absent_members_list = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    text_data = row['REPORTS']\n",
    "    dates.append(index)\n",
    "    \n",
    "    time_match = re.search(r\"(\\d{1,2}:\\d{2}:\\d{2} (AM|PM))\", text_data)\n",
    "    meeting_time = time_match.group(1) if time_match else None\n",
    "    times.append(meeting_time)\n",
    "    \n",
    "    present_match = re.search(r\"Members Present:(.*?)Members Absent\", text_data, re.DOTALL)\n",
    "    present_members = present_match.group(1).strip() if present_match else None\n",
    "    present_members_list.append(present_members)\n",
    "    \n",
    "    absent_match = re.search(r\"Members Absent:(.*?)Members Late\", text_data, re.DOTALL)\n",
    "    absent_members = absent_match.group(1).strip() if absent_match else None\n",
    "    absent_members_list.append(absent_members)\n",
    "\n",
    "parsed_data = {\n",
    "    \"Date\" : dates,\n",
    "    \"Time\": times,\n",
    "    \"Present Members\": present_members_list,\n",
    "    \"Absent Members\": absent_members_list\n",
    "}\n",
    "\n",
    "parsed_df = pd.DataFrame(parsed_data)\n",
    "\n",
    "######################################################################################################################################\n",
    "\n",
    "# pipe = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "\n",
    "hyperparameters = {\n",
    "    'model_name_or_path': 'deepset/roberta-base-squad2',\n",
    "    'output_dir': '/opt/ml/model',\n",
    "    'batch_size': 96,\n",
    "    'n_epochs': 2,\n",
    "    'base_LM_model': \"roberta-base\",\n",
    "    'max_seq_len': 386,\n",
    "    'learning_rate': 3e-5,\n",
    "    'lr_schedule': 'LinearWarmup',\n",
    "    'warmup_proportion': 0.2,\n",
    "    'doc_stride': 128,\n",
    "    'max_query_length': 64\n",
    "}\n",
    "\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git', 'branch': 'v4.37.0'}\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='run_qa.py',\n",
    "    source_dir='',\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    git_config=git_config,\n",
    "    transformers_version='4.37.0',\n",
    "    pytorch_version='2.1.0',\n",
    "    py_version='py310',\n",
    "    hyperparameters=hyperparameters\n",
    ")\n",
    "\n",
    "huggingface_estimator.fit()\n",
    "\n",
    "def interact_with_chatbot(user_input, conversation_history):\n",
    "    conversation_history.append(f\"User: {user_input}\")\n",
    "    \n",
    "    conversation_text = \" \".join(conversation_history[-5:])\n",
    "    \n",
    "    result = pipe(question=user_input, context=conversation_text)\n",
    "    \n",
    "    response_text = result['answer']\n",
    "    \n",
    "    conversation_history.append(f\"Chatbot: {response_text}\")\n",
    "    \n",
    "    return response_text\n",
    "\n",
    "def delete_model_files():\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")\n",
    "    \n",
    "    if os.path.exists(cache_dir):\n",
    "        user_input = input(\"Do you want to delete the model files from the cache directory? (y/n): \")\n",
    "        if user_input.lower() == 'y':\n",
    "            shutil.rmtree(cache_dir)\n",
    "            print(f\"Deleted model files from cache directory: {cache_dir}\")\n",
    "        else:\n",
    "            print(\"Model files not deleted from cache directory.\")\n",
    "    else:\n",
    "        print(f\"Model files not found in cache directory: {cache_dir}\")\n",
    "\n",
    "atexit.register(delete_model_files)\n",
    "\n",
    "print(\"Welcome to the Miami Report Chatbot!\")\n",
    "print(\"Type 'quit' to end the conversation.\\n\")\n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    \n",
    "    if user_input.lower() == 'quit':\n",
    "        print(\"Thank you for using the Miami Report Chatbot. Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    response = interact_with_chatbot(user_input, conversation_history)\n",
    "    print(f\"Chatbot: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4282df-4339-4e66-83ed-b57ed7cbc8f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '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' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m 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\n",
      "\u001b[0;31mNameError\u001b[0m: name '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' is not defined"
     ]
    }
   ],
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5683f2d6-3ec3-44e9-834c-66ce90b3e1a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_pinecone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharacterTextSplitter\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_pinecone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PineconeVectorStore\n\u001b[1;32m      8\u001b[0m load_dotenv()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_pinecone'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ingesting data...\")\n",
    "    \n",
    "    # load pdf document\n",
    "    loader = PyPDFLoader(\"data/impact_of_generativeAI.pdf\")\n",
    "    document = loader.load()\n",
    "    \n",
    "    # split entire documents into chunks  \n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    texts = text_splitter.split_documents(document)\n",
    "    print(f\"created {len(texts)} chunks\")\n",
    "\n",
    "    # create vector embeddings and save it in pinecone database\n",
    "    embeddings = OpenAIEmbeddings(openai_api_type=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    PineconeVectorStore.from_documents(texts, embeddings, index_name=os.environ.get(\"INDEX_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee4cf7a-c5f5-4471-8bf9-a95242d71af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd51db4c-595e-4736-a2e8-0d740c9178d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_pinecone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharacterTextSplitter\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_pinecone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PineconeVectorStore\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_pinecone'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09ff6b-b5e9-448b-8943-0418923eb89e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
